---
layout: post
title: "The Semantic Layer"
date: 2026-02-07 00:02:00 -0500
author: Cody
tags: [ai, communication, technology, philosophy]
categories: [Technology]
---

<audio controls style="width: 100%; margin-bottom: 1.5em;">
  <source src="{{ site.baseurl }}/assets/audio/the-semantic-layer.mp3" type="audio/mpeg">
</audio>

Something strange is happening to human communication.

A person has an idea. It's fuzzy, half-formed. They type a few bullet points into an AI, which expands them into a polished document. The document gets sent. The recipient uses AI to summarize it back into bullets.

Fuzzy idea → AI expansion → AI compression → received understanding.

What actually got communicated?

This isn't like auto-tune or a beauty filter. Those smooth an existing signal. This is different. The AI adds information, shapes the direction, fills gaps with its own predictions. And then on the receiving end, another AI strips it back down, deciding what's essential.

The LLM is becoming a shared epistemological layer. Ideas pass through it in both directions. And it's not neutral.

Consider: if both parties use the same model, they might think they understand each other perfectly while actually just agreeing on the distortion. The "communication" that survives is whatever the AI considers plausible, not necessarily what either person meant.

You can already run any article through a "remove opinion" filter and extract just the facts. But who decides what counts as fact versus opinion? The model does. You've outsourced your epistemology.

If everyone uses the same few models, does discourse converge toward the center of the training distribution? Does heterodox thinking get smoothed out? Or does competition between models create ideological forks, different people using different AIs inhabiting different realities?

This breaks the old communication model. It's not just noise in the channel. It's active transformation. The medium is no longer neutral. It has tendencies, training, guardrails.

Whoever controls the transformation function controls how ideas take shape before transmission and how they're interpreted on receipt. That's not editorial control. It's pre-editorial. It's deeper.

We used to worry about AI generating content. The bigger shift might be AI becoming the medium through which all human ideas pass. Not the author. The translator.

And translators always change the meaning.

---

## Sources & Further Reading

- [Shannon-Weaver Model of Communication](https://en.wikipedia.org/wiki/Shannon%E2%80%93Weaver_model) — The classic model this breaks
- [The Medium is the Message](https://en.wikipedia.org/wiki/The_medium_is_the_message) — McLuhan's insight, more relevant than ever
- [Epistemic Bubbles and Echo Chambers](https://journals.sagepub.com/doi/10.1177/1529100612451018) — How information environments shape belief
